{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization Frameworks\n",
    "> An comparison of packages for Bayesian optimization in Python (and R).\n",
    "\n",
    "- toc: true\n",
    "- categories: [Bayesian optimization]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Optimization (BO) is a method for optimizing expensive black-box functions by means of a probabilistic surrogate model and an acquisition function that specifies a trade-off between exploration (improving the surrogate) and exploitation (finding the minimizer of the surrogate and thus of the black-box function).\n",
    "\n",
    "As **surrogate model** typically Gaussians processes (GP) or tree-based models (RF, GBT ...) are used.\n",
    "These have their own parameters that need to be estimated via maximum likelihood (ML) or integrated out.\n",
    "The ML point estimate is typically searched for using a gradient-based local optimizer together with a global optimization heuristic such as multiple restarts to address the non-convexiy of the problem.\n",
    "For integrating out the model parameters either Markov-chain Monte Carlo (MCMC) or approximate variational inference (VI) methods are used, together with an assumption on the prior distribution.\n",
    "\n",
    "**Search spaces** define the possible inputs.\n",
    "When applying BO for tuning hyperparameters of machine learning models the inputs include continuous, discrete and categorical variables and may include conditionals (think of having to choose beta1 and beta2 when selecting Adam).\n",
    "When applying BO to optimization of real-world processes there are typically some (in-)equality constraints that need to be satisfied.\n",
    "\n",
    "**Acquistion functions** either have a analytic form, or need to be approximated via Monte Carlo methods.\n",
    "We want to find the minimizer of the acquistion over the search space in order to evaluate it next.\n",
    "This is typically done using a gradient-free or gradient-based local optimizer or via MC-sampling, depending on the nature of the acquistion function.\n",
    "The search space including its constraints needs to be handled by the optimizer.\n",
    "In multi-objective BO the black-box function has multiple outputs. \n",
    "Here the acquisition function needs to guide the search towards exploring the Pareto front. \n",
    "An comparison of acquisition functions is given here: [single-objective](https://davidwalz.github.io/blog/bayesian%20optimization/2020/06/20/bayesopt-acquisitions-single.html), [multi-objective](https://davidwalz.github.io/blog/bayesian%20optimization/2020/06/20/bayesopt-acquisitions-multi.html).\n",
    "\n",
    "There are a number of general-purpose (not focussing only on hyperparameter tuning) BO frameworks available in Python and R. \n",
    "In this post the main frameworks are compared in terms of supported features and development & support activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature comparison\n",
    "\n",
    "All packages in this comparison support GP surrogates with analytic acquisitions EI/PI/CB over continuous search spaces.\n",
    "Beyond that it's interesting to note that no package currently provides all options in terms of surrogates, acquistions and search space, so it really depends on the type of problem you want to apply BO on.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | Surrogates | Hyperparameter handling | Single-objective acquisitions | Multi-objective acquisitions | Search space |\n",
    "| ------------------------------------------------------------------------- | ------------------------------------------------------------------------ | ----------------------------------------------------- | ---------------------------- | ----------------------------------------------- | --------------- |\n",
    "| [mlrMBO](https://mlr-org.github.io/mlrMBO) | GP, RF ([mlr](https://mlr.mlr-org.com/)) | ML | EI, CB, AEI, EQI, AdaCB | DIB | continuous, integer, categorical + constraints |\n",
    "| [pyGPGO](http://pygpgo.readthedocs.io) | GP (native), GBT, RF, ET ([scikit-learn](https://scikit-learn.org)) | ML, MCMC ([pymc3](https://docs.pymc.io/)) | EI, PI, CB | | continuous, integer |\n",
    "| [acikit-optimize](https://scikit-optimize.github.io) | GP, RF, GBT ([scikit-learn](https://scikit-learn.org)) | ML | EI, PI, CB | - | continuous, discrete, categorical + constraints | \n",
    "| [GPyOpt](https://github.com/SheffieldML/GPyOpt) | GP ([GPy](http://sheffieldml.github.io/GPy)) | ML, MCMC | EI, PI, CB | - | continuous, discrete, categorical + constraints | \n",
    "| [GPflowOpt](https://github.com/GPflow/GPflowOpt) | GP ([GPflow](https://github.com/GPflow/GPflow)) | ML | EI, PI, CB, MES, PF | HVPI | continuous |\n",
    "| [BoTorch](https://botorch.org/) | GP ([GPyTorch](https://github.com/cornellius-gp/gpytorch)), extensible | ML, MCMC (Pyro)| EI, PI, CB, qMES, qKG, extensible | custom scalarizations, HVEI | continous + linear constraints |\n",
    "| [Emukit](https://github.com/amzn/emukit) | GP ([GPy](http://sheffieldml.github.io/GPy)), extensible | ML | EI, PI, CB, ES, MES, PF | - | continuous, integer, categorical + constraints |\n",
    "| [DragonFly](https://github.com/dragonfly/dragonfly) | GP (native) | ML, posterior sampling | EI, CB, PI, TTEI, TS | scalarization with CB/TS | continuous, categorical |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity comparison\n",
    "\n",
    "The following table gives an impression on the popularity and development activity of the frameworks, based on github statistics. \n",
    "The top-3 in terms of stars, contributors, commits and issues are scikit-optimize, botorch (together with Ax which builds on botorch) and mlrMBO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                 stars  forks  contributors  commits  open_issues  \\\nname                                                                \nscikit-optimize   1836    349            55     1500          150   \nbotorch           1630    137            28      656           24   \nmlrMBO             162     40            14     1600           84   \nAx                1179    113            46      608           21   \nemukit             218     64            20      282           31   \nGPyOpt             621    190            35      504           89   \nGPflowOpt          213     51             5      426           22   \ndragonfly          498     57             8      393           17   \npyGPGO             182     47             2      292            7   \n\n                 closed_issues     created last_commit       license  \nname                                                                  \nscikit-optimize            769  2016-03-20  2020-05-18  BSD-3-Clause  \nbotorch                    437  2018-07-30  2020-06-24           MIT  \nmlrMBO                     410  2013-10-23  2020-06-15   NOASSERTION  \nAx                         318  2019-02-09  2020-06-29           MIT  \nemukit                     278  2018-09-04  2020-06-03    Apache-2.0  \nGPyOpt                     235  2014-08-13  2020-03-19  BSD-3-Clause  \nGPflowOpt                   97  2017-04-28  2018-09-12    Apache-2.0  \ndragonfly                   49  2018-04-20  2020-03-13           MIT  \npyGPGO                      18  2016-11-23  2019-06-15           MIT  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stars</th>\n      <th>forks</th>\n      <th>contributors</th>\n      <th>commits</th>\n      <th>open_issues</th>\n      <th>closed_issues</th>\n      <th>created</th>\n      <th>last_commit</th>\n      <th>license</th>\n    </tr>\n    <tr>\n      <th>name</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>scikit-optimize</th>\n      <td>1836</td>\n      <td>349</td>\n      <td>55</td>\n      <td>1500</td>\n      <td>150</td>\n      <td>769</td>\n      <td>2016-03-20</td>\n      <td>2020-05-18</td>\n      <td>BSD-3-Clause</td>\n    </tr>\n    <tr>\n      <th>botorch</th>\n      <td>1630</td>\n      <td>137</td>\n      <td>28</td>\n      <td>656</td>\n      <td>24</td>\n      <td>437</td>\n      <td>2018-07-30</td>\n      <td>2020-06-24</td>\n      <td>MIT</td>\n    </tr>\n    <tr>\n      <th>mlrMBO</th>\n      <td>162</td>\n      <td>40</td>\n      <td>14</td>\n      <td>1600</td>\n      <td>84</td>\n      <td>410</td>\n      <td>2013-10-23</td>\n      <td>2020-06-15</td>\n      <td>NOASSERTION</td>\n    </tr>\n    <tr>\n      <th>Ax</th>\n      <td>1179</td>\n      <td>113</td>\n      <td>46</td>\n      <td>608</td>\n      <td>21</td>\n      <td>318</td>\n      <td>2019-02-09</td>\n      <td>2020-06-29</td>\n      <td>MIT</td>\n    </tr>\n    <tr>\n      <th>emukit</th>\n      <td>218</td>\n      <td>64</td>\n      <td>20</td>\n      <td>282</td>\n      <td>31</td>\n      <td>278</td>\n      <td>2018-09-04</td>\n      <td>2020-06-03</td>\n      <td>Apache-2.0</td>\n    </tr>\n    <tr>\n      <th>GPyOpt</th>\n      <td>621</td>\n      <td>190</td>\n      <td>35</td>\n      <td>504</td>\n      <td>89</td>\n      <td>235</td>\n      <td>2014-08-13</td>\n      <td>2020-03-19</td>\n      <td>BSD-3-Clause</td>\n    </tr>\n    <tr>\n      <th>GPflowOpt</th>\n      <td>213</td>\n      <td>51</td>\n      <td>5</td>\n      <td>426</td>\n      <td>22</td>\n      <td>97</td>\n      <td>2017-04-28</td>\n      <td>2018-09-12</td>\n      <td>Apache-2.0</td>\n    </tr>\n    <tr>\n      <th>dragonfly</th>\n      <td>498</td>\n      <td>57</td>\n      <td>8</td>\n      <td>393</td>\n      <td>17</td>\n      <td>49</td>\n      <td>2018-04-20</td>\n      <td>2020-03-13</td>\n      <td>MIT</td>\n    </tr>\n    <tr>\n      <th>pyGPGO</th>\n      <td>182</td>\n      <td>47</td>\n      <td>2</td>\n      <td>292</td>\n      <td>7</td>\n      <td>18</td>\n      <td>2016-11-23</td>\n      <td>2019-06-15</td>\n      <td>MIT</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "#hide_input\n",
    "import util\n",
    "\n",
    "df = util.compare_repos([\n",
    "    'mlr-org/mlrMBO',\n",
    "    'SheffieldML/GPyOpt',\n",
    "    'scikit-optimize/scikit-optimize',\n",
    "    'GPflow/GPflowOpt',\n",
    "    'pytorch/botorch',\n",
    "    'amzn/emukit',\n",
    "    'dragonfly/dragonfly',\n",
    "    'josejimenezluna/pyGPGO',\n",
    "    'facebook/Ax'\n",
    "])\n",
    "df.sort_values(by='closed_issues', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPyOpt\n",
    "* GPy [[code]](https://github.com/SheffieldML/GPy) [[doc]](https://gpy.readthedocs.io/en/latest/)\n",
    "* GPyOpt [[code]](https://github.com/SheffieldML/GPyOpt) [[doc]](https://gpyopt.readthedocs.io/en/latest/)\n",
    "\n",
    "GPyOpt is a BO package built on top of the hugely popular GPy for flexible GP modeling. Both are being developed by the university of Sheffield.\n",
    "Together with mlrMBO, GPyOpt has been around the longest. However, development of GPyOpt has somewhat stalled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Optimize\n",
    "[[code]](https://github.com/scikit-optimize/scikit-optimize)\n",
    "[[doc]](https://scikit-optimize.github.io/)\n",
    "\n",
    "Scikit-Optimize is an actively developed and well polished BO package based on the GP and tree-based models in Scikit-Learn. Not supported are model parameter integration and multi-objective optimization. Compared to GPy, the GP modeling in Scikit-Learn is rather rudimentary. Mixed search spaces and constraints are supported, as well as external, delayed and batched evaluations. For Hyperparameter tuning in Scikit-Learn there is a drop-in replacement for Grid/RandomSearchCV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPFlowOpt (with TF & GPFlow)\n",
    "* GPFlow [[code]](https://github.com/GPflow/GPflow) [[doc]](https://gpflow.readthedocs.io/en/latest/) [[paper]](https://arxiv.org/abs/1711.03845)\n",
    "* GPFlowOpt [[code]](https://github.com/GPflow/GPflowOpt) [[doc]](https://gpflowopt.readthedocs.io/en/latest/) [[paper]](http://jmlr.org/papers/v18/16-537.html)\n",
    "\n",
    "GPFlowOpt is package built on top of GPFlow, which in turn uses TensorFlow for fast linear algebra computations with GPU-support and auto-differentiation. This makes it more extensible as different models and acquisition functions can be implemented without having to define gradients for the optimizer.\n",
    "The top-level API is inspired GPy.\n",
    "\n",
    "*Development on GPFlowOpt seems to have stopped since the end of 2018.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoTorch (with PyTorch, GPyTorch & Pyro)\n",
    "* BoTorch [[code]](https://github.com/pytorch/botorch) [[doc]](https://botorch.org/) [[paper]](https://arxiv.org/abs/1910.06403)\n",
    "* GPyTorch [[code]](https://github.com/cornellius-gp/gpytorch) [[doc]](https://gpytorch.ai/) [[paper]](https://arxiv.org/abs/1809.11165)\n",
    "* Pyro [[code]](https://github.com/pyro-ppl/pyro) [[doc]](http://docs.pyro.ai) [[paper]](https://arxiv.org/abs/1810.09538)\n",
    "\n",
    "BoTorch is a package built on top of GPyTorch (GP modeling), Pyro (MCMC and variational inference) and PyTorch as its compution framework. Hence, it profits from GPU support and auto-differentiation. BoTorch is extremely flexible, e.g. in it's first class support for custom acquisition functions, which is made possible by MC integration (quasi-MC together with the reparametrisation trick) and auto-differentiation for gradient-based optimization. BoTorch supports:\n",
    "* GP models (multi-fidelity, multi-task, ...), variational neural networks \n",
    "* MC handling of model parameters is in principle supported via GPyTorch and Pyro, but is not yet implemented\n",
    "* analytic acquisitions (EI, PI, CB) and MC acquisitions: (knowledge gradient, max-value entropy search, posterior variance), cost awareness and custom acquisitions\n",
    "* multi-objective optimization via passing a custom pytorch function that scalarizes the objectives to the corresponding MC acqusisition\n",
    "* only continous search spaces; categorical / ordinal variables need to be encoded beforehand\n",
    "* parameter constraints (linear inequality constraints) and outcome constraints\n",
    "* batched proposals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ax\n",
    "[[code]](https://github.com/facebook/Ax)\n",
    "[[doc]](https://ax.dev)\n",
    "[[test]](ax.ipynb)\n",
    "\n",
    "Ax is a high-level framework for Bayesian and Bandit optimization. \n",
    "For BO, Ax relies mostly on BoTorch from the same developers, hence the entire feature set of BoTorch is available in principle, but needs to implemented first.\n",
    "For Bandits optimization, Thompson sampling is used.\n",
    "Ax provides:\n",
    "* a service-like API for managing and storing experiments as JSON files (local) or in SQL databases. The service is only running locally.\n",
    "* transform pipelines to handle encoding of categorical and ordinal variables, scaling and log-transforms\n",
    "* limited support for parameter constraints of type $x1 \\leq x2$ and $\\sum x_i \\leq c$ as well as output constraints\n",
    "* limited support for multi-objective optimization: only weighted sum scalarization and no tooling around it\n",
    "* examples for human-in-the-loop optimization\n",
    "\n",
    "Overall, I think Ax needs more maturing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emukit\n",
    "[[code]](https://github.com/amzn/emukit)\n",
    "[[doc]](https://amzn.github.io/emukit/)\n",
    "[[paper]]()\n",
    "\n",
    "Emukit is a high-level framework for Bayesian optimization and and Bayesian quadrature. It is intended to be independent of the modeling framework, but supports first class support for GPy. Similar built-in support for other frameworks is apparently not planned. Mixed search spaces and constraints are supported, multi-objective optimization is currently not.\n",
    "\n",
    "Emukit provides abstraction layers for the individual components of Bayesian optimization in order to implement algorithms independent of the concrete modeling framework. While the idea is intriguing, concepts like auto-differentiation that make GPFlowOpt and GPyTorchOpt powerfull fall short here. Instead, by focussing on GPy, Emukit seems to end up as replacement for GPyOpt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dragonfly\n",
    "[[code]](https://github.com/dragonfly/dragonfly)\n",
    "[[doc]](https://dragonfly-opt.readthedocs.io)\n",
    "[[paper]](https://arxiv.org/abs/1903.06694)\n",
    "\n",
    "Dragonfly is package developed at Carneggie Melon University. It has native implementations of GPs with the typical kernels, an optimizer (DOO), MCMC samplers copied from copied from pymc3 and pgmpy (Metropolis, Slice, NUTS, HMC). Multi-objective optimization is supported via random scalarizations, constraints are not.\n",
    "Note that Thompson sampling from GPs seems to be incorrectly implemented, as points are sampled without keeping track of the previously sampled points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other projects\n",
    "* [fmfn/BayesianOptimization](https://github.com/fmfn/BayesianOptimization) - Small BO package using GPs from Scikit-Learn \n",
    "* [Cornell-MOE](https://github.com/wujian16/Cornell-MOE) - Relatively old Python / C++ package for BO\n",
    "* [ProcessOptimizer](https://github.com/bytesandbrains/ProcessOptimizer) - Fork of Scikit-Optimizer for optimizing real world processes. Provides classes for composable constraints. However, only rejection sampling is implemented.\n",
    "* [Phoenics](https://github.com/aspuru-guzik-group/phoenics) - BO package using kernel density estimates and a specific multi-objective acquistion called CHIMERA\n",
    "* [TS-EMO](https://github.com/Eric-Bradford/TS-EMO) - Matlab implementation of the TS-EMO algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}